{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils as utils\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     # random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_agg = pd.read_csv('../../../train_data_revision1/aggregation-weight.txt',header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1 = 12\n",
    "# num_2 -> hid_1\n",
    "num_2 = 8\n",
    "num_3 = 16\n",
    "# hid_2\n",
    "hid_2 = 24\n",
    "hid_dim_2 = 128\n",
    "lstm_num_layer = 2\n",
    "epochs = 500\n",
    "drop_rate = 0.5\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "wdr = 0.0001\n",
    "gcn_hid_dim = 8\n",
    "hid_dim = 36\n",
    "device = 'cuda'\n",
    "n_his = 21\n",
    "n_pred = 1\n",
    "community_num = 139\n",
    "step_size = 10\n",
    "gamma = 0.996\n",
    "file_path = '../../../train_data_revision1/'\n",
    "data_path = file_path + 'cbg_input.npy'\n",
    "cbg_edge_path = file_path + 'edge_21day.npy'\n",
    "model_save_path = 'covid-21daysconv-weighted.pth'\n",
    "model_read_path = ''\n",
    "infect_data_path = file_path + 'number_of_infections_1day.csv'\n",
    "cbg_to_com_file_path = file_path + 'cbg_to_comm.txt'\n",
    "scaler = MinMaxScaler()\n",
    "scaler_inf = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totensor(data):\n",
    "    return torch.Tensor(data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, data_iter):\n",
    "    model.eval()\n",
    "    l_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for cbg_data,inf_data,edge_list,y in data_iter:\n",
    "            b,c,_ = inf_data.size()\n",
    "            y_pred = model(cbg_data,inf_data,edge_list).reshape(b,c)\n",
    "            y = y.reshape(len(y),-1)\n",
    "            l = loss(y_pred, y)\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "        return l_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metric(model, data_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mae, sum_y, mape, mse = [], [], [], []\n",
    "        for cbg_data,inf_data,edge_list,y in data_iter:\n",
    "            y = y.reshape(len(y),-1)\n",
    "            y = scaler_inf.inverse_transform(y.cpu().numpy().reshape(-1,1)).reshape(-1)\n",
    "            \n",
    "            b,c,_ = inf_data.size()\n",
    "            y_pred = model(cbg_data,inf_data,edge_list).reshape(b,c)\n",
    "            \n",
    "            y_pred = scaler_inf.inverse_transform(y_pred.view(-1,1).cpu().numpy()).reshape(-1)\n",
    "            d = np.abs(y - y_pred)\n",
    "            mae += d.tolist()\n",
    "            sum_y += y.tolist()\n",
    "            mse += (d ** 2).tolist()\n",
    "        MAE = np.array(mae).mean()\n",
    "        RMSE = np.sqrt(np.array(mse).mean())\n",
    "        WMAPE = np.sum(np.array(mae)) / np.sum(np.array(sum_y))\n",
    "        return MAE, RMSE, WMAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbg_to_com_file = pd.read_csv(cbg_to_com_file_path)\n",
    "def cbg_to_com(tensor_1,file):\n",
    "    n_row = tensor_1.shape[0]\n",
    "    dim = tensor_1.shape[2]\n",
    "    data_2 = torch.zeros((n_row,139,dim)).to(device)\n",
    "    for i in range(file.shape[0]):\n",
    "        index_cbg = file.iloc[i,0]\n",
    "        index_com = file.iloc[i,1]\n",
    "        data_2[:,index_com] += tensor_1[:,index_cbg] * weight_agg[index_cbg][0]\n",
    "    return data_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_i, n_h, n_o):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_i, n_h)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(n_h, n_o)\n",
    "    def forward(self, input):\n",
    "        input = self.linear1(input)\n",
    "        input = self.relu(input)\n",
    "        input = F.dropout(input, p=drop_rate, training=self.training)\n",
    "        input = self.linear2(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,hid_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim,add_self_loops=True)\n",
    "        self.conv2 = GCNConv(hid_dim, out_dim,add_self_loops=True)\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=drop_rate, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=drop_rate, training=self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Covid_gcn_pre(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Covid_gcn_pre,self).__init__()\n",
    "        self.input_mlp = MLP(3,hid_dim,num_1)\n",
    "        self.cbg_gcn = GCN(num_1,num_2,gcn_hid_dim)\n",
    "        self.inf_mlp = nn.Linear(1,num_3)\n",
    "        self.hid_2_linear = nn.Linear(num_2+num_3,hid_2)\n",
    "        self.output_mlp = MLP(hid_dim*n_his,hid_dim_2,1)\n",
    "        self.rnn = nn.LSTM(hid_2, hid_dim, lstm_num_layer, batch_first = True)\n",
    "    def forward(self,cbg,inf,edge):\n",
    "        b,c,w,f = cbg.size()\n",
    "        cbg = self.input_mlp(cbg)\n",
    "        cbg_output = torch.Tensor().to(device)\n",
    "        for j in range(b):\n",
    "            cbg_edge_index = edge[j][0:2].long()\n",
    "            cbg_edge_weight = edge[j][2]\n",
    "            cbg_output_1 = torch.Tensor().to(device)\n",
    "            for i in range(n_his):\n",
    "                cbg_tmp = cbg[j,:,i,:].reshape(1,c,-1)\n",
    "                cbg_tmp = self.cbg_gcn(cbg_tmp,cbg_edge_index,cbg_edge_weight).reshape(1,c,1,-1)\n",
    "                cbg_output_1 = torch.cat([cbg_output_1,cbg_tmp],2)\n",
    "            cbg_output = torch.cat([cbg_output,cbg_output_1],0)\n",
    "        cbg_output = cbg_output.reshape(b,c,-1)\n",
    "        cbg_output = cbg_to_com(cbg_output,cbg_to_com_file).reshape(-1,n_his,num_2)\n",
    "        inf = inf.reshape(-1,n_his,1)\n",
    "        inf = self.inf_mlp(inf)\n",
    "        temp = torch.cat([cbg_output,inf],axis=2)\n",
    "        temp = self.hid_2_linear(temp)\n",
    "        temp,_ = self.rnn(temp)\n",
    "        temp = temp.reshape(-1,139,hid_dim*n_his)\n",
    "        output = self.output_mlp(temp)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Covid_gcn_pre().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate,val_rate,test_rate = 0.5,0.2,0.3\n",
    "len_record = 300\n",
    "num = len_record - n_his\n",
    "len_train = int(num*train_rate)\n",
    "len_val = int(num*val_rate)\n",
    "len_test = num - len_train - len_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算edge_index,edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge = np.load(cbg_edge_path)\n",
    "edge_train = totensor(edge[:len_train])\n",
    "edge_val = totensor(edge[len_train:len_val+len_train])\n",
    "edge_test = totensor(edge[len_val+len_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286, 2688, 14, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbg_data = np.load(data_path)\n",
    "weekly_pattern = cbg_data[:,:,0]\n",
    "popularity = cbg_data[:,:,1]\n",
    "vulnerability = cbg_data[:,:,2]\n",
    "weekly_pattern = scaler.fit_transform(weekly_pattern.reshape(-1,1)).reshape(cbg_data.shape[0],cbg_data.shape[1])\n",
    "popularity = scaler.fit_transform(popularity.reshape(-1,1)).reshape(cbg_data.shape[0],cbg_data.shape[1])\n",
    "vulnerability = scaler.fit_transform(vulnerability.reshape(-1,1)).reshape(cbg_data.shape[0],cbg_data.shape[1])\n",
    "temp_data = np.stack([weekly_pattern,popularity,vulnerability],axis=2)\n",
    "n_vertex = cbg_data.shape[1]\n",
    "len_record = len(cbg_data)\n",
    "x = np.zeros([num, n_his, n_vertex, 3])\n",
    "for i in range(num):\n",
    "    head = i\n",
    "    tail = i+n_his\n",
    "    x[i] = temp_data[head: tail]\n",
    "x = np.swapaxes(x,1,2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbg_train = totensor(x[:len_train])\n",
    "cbg_val = totensor(x[len_train:len_val+len_train])\n",
    "cbg_test = totensor(x[len_val+len_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### infection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 139, 14) (286, 139, 1)\n"
     ]
    }
   ],
   "source": [
    "infection_data = np.array(pd.read_csv(infect_data_path,index_col=0))\n",
    "infection_data = scaler_inf.fit_transform(infection_data.reshape(-1,1)).reshape(-1,139)\n",
    "X,Y = [],[]\n",
    "num = len(infection_data)-n_his\n",
    "for i in range(num):\n",
    "    X.append(infection_data[i:i+n_his])\n",
    "    Y.append(infection_data[i+n_his:i+n_his+n_pred])\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "X = np.swapaxes(X,1,2)\n",
    "Y = np.swapaxes(Y,1,2)\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "infection_train = totensor(X[:len_train])\n",
    "infection_val = totensor(X[len_train:len_val+len_train])\n",
    "infection_test = totensor(X[len_val+len_train:])\n",
    "y_train = totensor(Y[:len_train])\n",
    "y_val = totensor(Y[len_train:len_val+len_train])\n",
    "y_test = totensor(Y[len_val+len_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = utils.data.TensorDataset(cbg_train,infection_train,edge_train,y_train)\n",
    "val_data = utils.data.TensorDataset(cbg_val,infection_val,edge_val,y_val)\n",
    "test_data = utils.data.TensorDataset(cbg_test,infection_test,edge_test,y_test)\n",
    "train_iter = utils.data.DataLoader(dataset = train_data,batch_size=batch_size,shuffle=False)\n",
    "val_iter = utils.data.DataLoader(dataset = val_data,batch_size=batch_size,shuffle=False)\n",
    "test_iter = utils.data.DataLoader(dataset = test_data,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 57, 86)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data),len(val_data),len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(),lr=lr,weight_decay=wdr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=step_size,gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=20, verbose=False, delta=0 ,path = ''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''\n",
    "        Saves model when validation loss decrease.\n",
    "        验证损失减少时保存模型。\n",
    "        '''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), self.path)\n",
    "        # torch.save(model, self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "early_stopping = EarlyStopping(patience=8, verbose=True, path = model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = np.inf\n",
    "for epoch in tqdm_notebook(range(1,epochs + 1)):\n",
    "    l_sum, n = 0.0 , 0\n",
    "    for cbg_data,inf_data,edge_list,y in train_iter:\n",
    "        b,c,_ = inf_data.size()\n",
    "        y_pred = model(cbg_data,inf_data,edge_list).reshape(b,c)\n",
    "        y = y.reshape(len(y),community_num)\n",
    "        l = loss(y_pred,y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        l_sum += l.item() * y.shape[0]\n",
    "        n += y.shape[0]\n",
    "    val_loss = val(model, val_iter)\n",
    "    gpu_mem_alloc = torch.cuda.max_memory_allocated() / 1000000 if torch.cuda.is_available() else 0\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    print('Epoch: {:03d} | Lr: {:.20f} |Train loss: {:.8f} | Val loss: {:.8f} | GPU occupy: {:.8f} MiB '.\\\n",
    "          format(epoch, optimizer.param_groups[0]['lr'], l_sum / n, val_loss, gpu_mem_alloc))\n",
    "    if epoch > 150:\n",
    "        early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n",
    "print('\\nTraining finished.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_save_path))\n",
    "test_MAE, test_RMSE, test_WMAPE = evaluate_metric(model, test_iter)\n",
    "print(f'MAE {test_MAE:.6f} | RMSE {test_RMSE:.6f} | WMAPE {test_WMAPE:.8f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3860d6e0d0be45158f67b7497cd86fa5fc648422f0e508ab3756a6a3a2719315"
  },
  "kernelspec": {
   "display_name": "jpy-pytorch",
   "language": "python",
   "name": "jpy-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275.448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "775px",
    "left": "1213px",
    "right": "20px",
    "top": "108px",
    "width": "545px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
